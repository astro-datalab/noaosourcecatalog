#!/usr/bin/env python

import os
import sys
import numpy as np
import warnings
from astropy.io import fits
from astropy.utils.exceptions import AstropyWarning
from astropy.table import Table,vstack
import time
import shutil
import re
import subprocess
from glob import glob
import logging
import socket
from datetime import datetime
from argparse import ArgumentParser
from dlnpyutils import utils as dln,slurm
from nsc.nsc_instcal_measure import getnscdirs,Exposure


def taccifyname(filename):
    """ Modify mss filenames for tacc"""
    imagedir = '/scratch1/09970/dnidever/nsc/instcal/v4/images'
    if filename[:9]=='/net/mss1':
        base = os.path.basename(filename)
        instrument = base[:3] 
        night = '20'+base[4:10]
        newfilename = os.path.join(imagedir,instrument,night[:4],night,base)
    else:
        newfilename = filename
    return newfilename        

if __name__ == "__main__":
    # Run lots of NSC jobs
    parser = ArgumentParser(description='Run NSC jobs')
    parser.add_argument('stage', type=str, nargs=1, help='Stage (measure, calibrate or combine)')
    parser.add_argument('inputfile', type=str, nargs=1, help='Input list filename')
    parser.add_argument('version', type=str, nargs=1, help='NSC version')
    parser.add_argument('--host',type=str,nargs=1,default="None",help='hostname, default "None", other options supported are "cca","tempest_katie","tempest_group","gp09/7","tacc"')
    parser.add_argument('--partition',type=str,nargs=1,default='normal',help='what TACC partition to use')
    parser.add_argument('--maxtasks',type=int,nargs=1,default=20000,help='Maximum number of tasks to run')
    parser.add_argument('-r','--redo', action='store_true', help='Redo exposures that were previously processed')
    args = parser.parse_args()

    # Inputs                                        
    stage = args.stage[0].lower()
    inputfile = args.inputfile[0]
    version = args.version[0]                # NSC version, like "v4", default "None"
    if version=="None": version = None
    host = str(args.host[0])                 # hostname of server, default "None"                  
    if host=="None": host = None
    if isinstance(args.partition,list):
        partition = args.partition[0]
    else:
        partition = args.partition
    if isinstance(args.maxtasks,list):       # maximum number of tasks to run from input list
        maxtasks = args.maxtasks[0]
    else:
        maxtasks = args.maxtasks
    redo = args.redo                         # if called, redo = True

    print('Input Parameters:')
    print('-----------------')
    print('stage =',stage,)
    print('inputfile =',inputfile)
    print('version =',version)
    print('host =',host)
    print('maxtasks =',maxtasks)
    print('redo =',redo)

    # Check that the input file exists
    if os.path.exists(inputfile)==False:
        print(inputfile,'NOT FOUND')
        sys.exit()

    # Load the input lines
    if inputfile.endswith('.fits') or inputfile.endswith('.fits.gz'):
        print('Loading',inputfile,' FITS file')
        inputdata = Table.read(inputfile)
    else:
        print('Loading',inputfile,' ASCII file')
        inputdata = dln.readlines(inputfile)
    print(len(inputdata),'inputs')

    # Get NSC directories                                                                                     
    basedir, tmpdir = getnscdirs(version,host)
    print("Working in basedir,tmpdir = ",basedir,tmpdir)
    # Make sure the directories exist                                                                         
    if not os.path.exists(basedir):
        os.makedirs(basedir)
    if not os.path.exists(tmpdir):
        os.makedirs(tmpdir)


    dt = [('cmd',str,1000),('name',str,1000),('output',str,1000),
          ('outfile',str,1000),('errfile',str,1000),('dir',str,1000)] 
    logtime = datetime.now().strftime("%Y%m%d%H%M%S")

    ##############################################################################
    # Measurement
    #============
    if stage=='meas' or stage=='measure' or stage=='measurement':
        script = 'nsc_instcal_measure'
        label = 'measure'
        if isinstance(inputdata,Table)==False:
            inputlist = inputdata.copy()
            dtinp = [('FLUXFILE',str,1000),('WTFILE',str,1000),('MASKFILE',str,1000)]
            inputdata = Table(np.zeros(len(inputlist),dtype=np.dtype(dtinp)))
            for i in range(len(inputlist)):
                ff,wt,mf = inputlist[i].split()
                inputdata['FLUXFILE'][i] = ff
                inputdata['WTFILE'][i] = wt
                inputdata['MASKFILE'][i] = mf

        # Check previously submitted tasks lists
        print('Checking previously submitted tasks lists')
        tasklist = glob(os.path.join(basedir,'lists',label+'_*_tasks.fits'))
        prevtasks = None
        for tf in tasklist:
            print('Loading ',tf)
            ptasks = Table.read(tf)
            if prevtasks is None:
                prevtasks = ptasks
            else:
                prevtasks = vstack((prevtasks,ptasks))
        # Some previous lists
        if prevtasks is not None:
            # Get unique exposures
            _,ui = np.unique(prevtasks['name'],return_index=True)
            prevtasks = prevtasks[ui]
            # Remove any previously submitted exposures
            inpbases = [os.path.basename(r['FLUXFILE']).replace('.fits.fz','') for r in inputdata]
            inpbases = np.array(inpbases).astype(str)
            _,ind1,ind2 = np.intersect1d(prevtasks['name'],inpbases,return_indices=True)
            if len(ind1)>0:
                print(len(ind1),' previously submitted exposures to remove')
                inputdata.remove_rows(ind2)

        # Loop over input data and populate the tasks list
        tasks = Table(np.zeros(np.minimum(len(inputdata),maxtasks),dtype=np.dtype(dt)))
        cnt = 0
        for i in range(len(inputdata)):
            fluxfile = inputdata['FLUXFILE'][i]
            wtfile = inputdata['WTFILE'][i]
            maskfile = inputdata['MASKFILE'][i]
            # Might need to modify mss filenames for TACC
            if host=='tacc':
                fluxfile = taccifyname(fluxfile)
                wtfile = taccifyname(wtfile)
                maskfile = taccifyname(maskfile)
            base = os.path.basename(fluxfile)
            if base.endswith('.fits.fz'): base=base[:-8]
            if base.endswith('.fits'): base=base[:-5]
            print('{:} {:}'.format(i+1,base))
            # Check that all three files exist
            infiles = [fluxfile,wtfile,maskfile]
            exists = [os.path.exists(f) for f in infiles]
            bd, = np.where(np.array(exists)==False)
            if len(bd)>0:
                print('Files not found: '+','.join(np.array(infiles)[bd])+'  SKIPPING')
                continue
            if base[:3] not in ['c4d','ksb','k4m']:
                print(base,'NOT in correct format. SKIPPING')
                continue
            instrument = base[:3]
            cmd = script+' '+fluxfile+' '+wtfile+' '+maskfile+' '+version
            if host:
                cmd += ' --host '+host
            # Check output filename
            if base[:3]=='c4d':
                night = '20'+base[4:10]
            else:
                instrument = 'c4d'  # assume decam for now
                head = fits.getheader(fluxfile,0)
                dateobs = head['DATE-OBS']
                night = dateobs[:4]+dateobs[5:7]+dateobs[8:10]
            outdir = os.path.join(basedir,instrument,night[:4],night,base)
            logfile = os.path.join(outdir,base+'.'+logtime+'.log')
            outfile = os.path.join(outdir,base+'_meas.fits')
            if os.path.exists(outfile) and redo==False:
                print(outfile,' ALREADY EXISTS.  Skipping')
                continue
            # Skip information in the tasks table
            tasks['cmd'][cnt] = cmd
            tasks['name'][cnt] = base
            tasks['output'][cnt] = outfile
            tasks['outfile'][cnt] = logfile 
            tasks['errfile'][cnt] = logfile.replace('.log','.err')
            tasks['dir'][cnt] = outdir
            cnt += 1
            if cnt>=maxtasks:
                print('Reached maxtasks ',maxtasks)
                break
        tasks = tasks[:cnt]  # trim

        # Calculate nodes needed
        # it takes about 1 hour per exposure on average
        # there are ~120 cores per node
        # we can only run for 48 hours
        # the tacc slurm queue limits launcher to a maximum of 56 tasks per node
        ntasks = np.minimum(len(tasks),maxtasks)
        total_time_hour = ntasks
        nodes = int(np.ceil(total_time_hour/56/48))

    ##############################################################################
    # Calibrate
    #==========
    elif stage=='calib' or stage=='calibrate':
        script = 'nsc_instcal_calibrate'
        label = 'calib'
        if isinstance(inputdata,Table)==False:
            inputlist = inputdata.copy()
            inputdata = Table(np.zeros(len(inputlist),dtype=np.dtype([('EXPOSURE',str,1000)])))
            inputdata['EXPOSURE'] = inputlist

        # Check previously submitted tasks lists
        print('Checking previously submitted tasks lists')
        tasklist = glob(os.path.join(basedir,'lists',label+'_*_tasks.fits'))
        prevtasks = None
        for tf in tasklist:
            print('Loading ',tf)
            ptasks = Table.read(tf)
            if prevtasks is None:
                prevtasks = ptasks
            else:
                prevtasks = vstack((prevtasks,ptasks))
        # Some previous lists
        if prevtasks is not None:
            # Get unique exposures
            _,ui = np.unique(prevtasks['name'],return_index=True)
            prevtasks = prevtasks[ui]
            # Remove any previously submitted exposures
            inpexposure = np.char.array(inputdata['EXPOSURE']).astype(str)
            _,ind1,ind2 = np.intersect1d(prevtasks['name'],inpexposure,return_indices=True)
            if len(ind1)>0:
                print(len(ind1),' previously submitted exposures to remove')
                inputdata.remove_rows(ind2)

        # Loop over input data and populate the tasks list
        tasks = Table(np.zeros(np.minimum(len(inputdata),maxtasks),dtype=np.dtype(dt)))
        cnt = 0
        for i in range(len(inputdata)):
            # "exposure" is the full path to the exposure output directory
            exposure = inputdata['EXPOSURE'][i]
            outdir = os.path.dirname(exposure)
            base = os.path.basename(exposure)
            print('{:} {:}'.format(i+1,base))
            cmd = script+' '+exposure+' '+version
            if host:
                cmd += ' --host '+host
            # Check output filename
            # we keep the meas.fits filename
            # need to check the table columns in the header
            measfile = os.path.join(outdir,base+'_meas.fits')
            if os.path.exists(measfile)==False:
                print('meas file',measfile,'NOT FOUND. Skipping')
                continue
            # Check that the meta file exists
            #metafile = os.path.join(outdir,base+'_meta.fits')
            #if os.path.exists(metafile) and 
            head = fits.getheader(measfile,1)
            # CHECK FOR THE NEEDED CALIBRATION COLUMNS
            # and check for the meta file

            # Requirements for "being done"
            # 1) meta file exists
            # 2) meas file exists
            # 3) meas file has calibration columns
            

            logfile = os.path.join(outdir,base+'_calib.log')
            outfile = measfile
            if os.path.exists(outfile) and redo==False:
                print(outfile,' ALREADY EXISTS.  Skipping')
                continue
            # Skip information in the tasks table
            tasks['cmd'][cnt] = cmd
            tasks['name'][cnt] = base
            tasks['output'][cnt] = outfile
            tasks['outfile'][cnt] = logfile 
            tasks['errfile'][cnt] = logfile.replace('.log','.err')
            tasks['dir'][cnt] = outdir
            cnt += 1
            if cnt>=maxtasks:
                print('Reached maxtasks ',maxtasks)
                break
        tasks = tasks[:cnt]  # trim

        # Calculate nodes need
        # calibration is pretty fast, maybe 1 min. per exposure
        # the tacc slurm queue limits launcher to a maximum of 56 tasks per node
        ntasks = np.minimum(len(tasks),maxtasks)
        total_time_hour = ntasks / 60.0
        nodes = int(np.ceil(total_time_hour/56/48))

    ##############################################################################
    # Combine
    #========
    elif stage=='combine':
        script = 'nsc_instcal_combine'
        label = 'combine'
        if isinstance(inputdata,Table)==False:
            inputlist = inputdata.copy()
            inputdata = Table(np.zeros(len(inputlist),dtype=np.dtype([('HEALPIX',str,1000)])))
            inputdata['HEALPILX'] = inputlist

        # Check previously submitted tasks lists
        print('Checking previously submitted tasks lists')
        tasklist = glob(os.path.join(basedir,'lists',label+'_*_tasks.fits'))
        prevtasks = None
        for tf in tasklist:
            print('Loading ',tf)
            ptasks = Table.read(tf)
            if prevtasks is None:
                prevtasks = ptasks
            else:
                prevtasks = vstack((prevtasks,ptasks))
        # Some previous lists
        if prevtasks is not None:
            # Get unique exposures
            _,ui = np.unique(prevtasks['name'],return_index=True)
            prevtasks = prevtasks[ui]
            # Remove any previously submitted exposures
            inphealpixpix = inputdata['HEALPIX']
            _,ind1,ind2 = np.intersect1d(prevtasks['name'],inphealpix,return_indices=True)
            if len(ind1)>0:
                print(len(ind1),' previously submitted exposures to remove')
                inputdata.remove_rows(ind2)

        # Loop over input data and populate the tasks list
        tasks = Table(np.zeros(np.minimum(len(inputdata),maxtasks),dtype=np.dtype(dt)))
        cnt = 0
        for i in range(len(inputdata)):
            healpix = inputdata['HEALPIX'][i]
            print('{:} {:}'.format(i+1,healpix))
            cmd = script+' '+healpix+' '+version
            if host:
                cmd += ' --host '+host
            # Check output filename
            outdir = os.path.join(basedir,'combine',str(int(healpix)//1000,),str(healpix))
            logfile = os.path.join(outdir,str(healpix)+'.log')
            outfile = os.path.join(outdir,str(healpix)+'.fits.gz')
            if os.path.exists(outfile) and redo==False:
                print(outfile,' ALREADY EXISTS.  Skipping')
                continue
            # Skip information in the tasks table
            tasks['cmd'][cnt] = cmd
            tasks['name'][cnt] = base
            tasks['output'][cnt] = outfile
            tasks['outfile'][cnt] = logfile 
            tasks['errfile'][cnt] = logfile.replace('.log','.err')
            tasks['dir'][cnt] = outdir
            cnt += 1
            if cnt>=maxtasks:
                print('Reached maxtasks ',maxtasks)
                break
        tasks = tasks[:cnt]  # trim

        # Calculate nodes need
        # the tacc slurm queue limits launcher to a maximum of 56 tasks per node
        # how much per healpix??
        ntasks = np.minimum(len(tasks),maxtasks)
        total_time_hour = ntasks
        nodes = int(np.ceil(total_time_hour/56/48))
    else:
        print('Stage ',stage,' not supported')
        sys.exit()

    print(len(tasks),' tasks to run')

    # Trim to maximum number of tasks
    if len(tasks)>maxtasks:
        print('Trimming to maximum tasks',maxtasks)
        tasks = tasks[:maxtasks]
    
    # Nodes to request
    print(nodes,'nodes requested')

    # Get walltime based on partition
    wtimedict = {'normal':'47:59:00','small':'47:59:00','development':'01:59:59'}
    walltime = wtimedict[partition]
    # nodes:
    # development 1-40
    # normal      3-512
    # small       2
    if partition=='development':
        if int(nodes)>40:
            print('development node limit is 40')
            nodes = 40
    elif partition=='normal':
        if int(nodes)<3:
            print('normal needs at least 3 nodes')
            nodes = 3
        if int(nodes)>512:
            print('normal node limit is 512')
            nodes = 512
    elif partition=='small':
        if int(nodes) != 2:
            print('small must be 2 nodes')
            nodes = 2

    # the tacc slurm queue limits launcher to a maximum of 56 tasks per node
    nparallel = nodes*56

    print('Slurm Parameters:')
    print('ntasks =',len(tasks))
    print('partition =',partition)
    print('nodes =',nodes)
    print('label =',label)
    print('walltime =',walltime)
    slurmroot = os.environ['SCRATCH']

    slurmdir,key = slurm.launcher(tasks,label,nodes=nodes,nparallel=nparallel,
                                  account=None,partition=partition,walltime=walltime,
                                  notification=True,stagger=True,
                                  slurmroot=slurmroot,verbose=True)
    # Write list of output directories
    dln.writelines(os.path.join(slurmdir,label+'_outdir.lst'),tasks['dir'].data.astype(str))
    # Write task list to lists/ 
    tasks.write(os.path.join(basedir,'lists',label+'_'+key+'_tasks.fits'))
    

    # submit(tasks,label,nodes=1,cpus=64,ppn=None,account='priority-davidnidever',
    #        partition='priority',shared=True,walltime='12-00:00:00',notification=False,
    #        memory=7500,numpy_num_threads=2,stagger=True,nodelist=None,precommands=None,
    #        verbose=True,logger=None):    


    # Start the logfile 
    #------------------ 
    #host = socket.gethostname()
    #hostname = host.split('.')[0]
    #logtime = datetime.now().strftime("%Y%m%d%H%M%S") 
    ## Set up logging to screen and logfile
    #logFormatter = logging.Formatter("%(asctime)s [%(levelname)-5.5s]  %(message)s")
    #logger = logging.getLogger() 
    #while logger.hasHandlers(): # some existing loggers, remove them   
    #    logger.removeHandler(logger.handlers[0]) 
    #logger = logging.getLogger()
    #logtime = datetime.now().strftime("%Y%m%d%H%M%S")
    #logfile = expdir+'/'+base+'_meas.log'
    #if os.path.exists(logfile): os.remove(logfile)
    #fileHandler = logging.FileHandler(logfile)
    #fileHandler.setFormatter(logFormatter)
    #logger.addHandler(fileHandler)
    #consoleHandler = logging.StreamHandler()
    #consoleHandler.setFormatter(logFormatter)
    #logger.addHandler(consoleHandler)
    #logger.setLevel(logging.NOTSET)





